{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd241782-3b7a-4904-89b2-04a37eac1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from util import *\n",
    "import random\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model, Input\n",
    "import copy as c\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4c9693-ecc2-464d-a25d-fb4181c10de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "seed = 420\n",
    "batch_size = 1024\n",
    "steps_per_epoch = 200\n",
    "nepochs = 500\n",
    "memlen = 64\n",
    "mem_size = 5000\n",
    "narrow_types = 4\n",
    "train_txt_fp = 'sym/songs/songs_train.txt'\n",
    "test_txt_fp = 'sym/songs/songs_test.txt'\n",
    "model_dir = 'trained_models'\n",
    "load_checkpoint = False\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56bb488-9935-4cd1-87b8-14d7234ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorify_from_fp_list(dataset_fp_list, memlen = 7, batch_size = 50, mem_size = 50000, shuffle = False, bidirectional_audio = True):\n",
    "    random.shuffle(dataset_fp_list)\n",
    "    def _gener():\n",
    "        k = 0\n",
    "        hopper = 0\n",
    "        song = None\n",
    "        while True:\n",
    "            while hopper < mem_size:     \n",
    "                with open(dataset_fp_list[k], 'rb') as f:\n",
    "                    loaded = pickle.load(f)\n",
    "                charts, _ = loaded[0], loaded[1]\n",
    "                del(loaded)\n",
    "                k = (k + 1) % (len(dataset_fp_list) - 1)\n",
    "                for chart in charts:\n",
    "                    newsong = [[a[0] for a in chart], [a[1] for a in chart], [a[1] for a in chart]]\n",
    "                    newsong[1] = [ddc_string_to_step(b) for b in newsong[1]]\n",
    "                    newsong[1] = windowize(np.array(newsong[1]), frames=memlen)\n",
    "                    newsong[0].append(0)\n",
    "                    newsong[0] = [[newsong[0][i], newsong[0][i+1], 0] for i in range(len(newsong[0])-1)]\n",
    "                    newsong[0][0][2] = 1\n",
    "                    newsong[0] = windowize(np.array(newsong[0]), frames = memlen)\n",
    "                    newsong[0] = np.concatenate((newsong[1],newsong[0]), axis = -1)\n",
    "                    if song is None:\n",
    "                        song = newsong\n",
    "                    else:\n",
    "                        for j in range(3):\n",
    "                            song[j] = np.append(song[j],newsong[j], axis = 0)\n",
    "                    hopper += len(newsong[0])\n",
    "\n",
    "                if shuffle == True:\n",
    "                    for i in range(3):\n",
    "                        np.random.seed(seed)\n",
    "                        song[i] = np.random.permutation(song[i])\n",
    "                gc.collect()\n",
    "            gc.collect()\n",
    "                \n",
    "            assert len(song[0])>batch_size+memlen\n",
    "\n",
    "            success_take = 0\n",
    "            miss_take = 0\n",
    "            sd = []\n",
    "            lb = []\n",
    "            i = 0\n",
    "            while success_take<batch_size:\n",
    "                sd.append(list(song[0][i][:-1])+[[0 for j in range(narrow_types*4)]+list(song[0][i][-1][-3:])])    \n",
    "                lb.append(sparse_to_categorical(sparceify([int(a) for a in list(song[2][i])]), 255))\n",
    "                success_take += 1\n",
    "                i += 1\n",
    "            sd, lb = np.array(sd), np.array(lb)\n",
    "            \n",
    "            for j in range(3):\n",
    "                song[j] = song[j][int(batch_size + miss_take):]\n",
    "            hopper -= batch_size\n",
    "            yield sd, lb\n",
    "    return _gener()\n",
    "\n",
    "\n",
    "\n",
    "def get_inputs_and_gens(trn_fp, tst_fp, shuffle = False, batch_size = 1000, bidirectional_audio = True, memlen = 64, aud_memlen = 15):\n",
    "    trn_ds, tst_ds = get_dataset_fp_list(trn_fp, tst_fp)\n",
    "\n",
    "    train_gen = generatorify_from_fp_list(trn_ds, batch_size=batch_size, shuffle = shuffle, mem_size=mem_size, memlen=memlen, aud_memlen=aud_memlen, bidirectional_audio=bidirectional_audio)\n",
    "    test_gen = generatorify_from_fp_list(tst_ds, batch_size=batch_size, shuffle = shuffle, mem_size=mem_size, memlen=memlen, aud_memlen=aud_memlen, bidirectional_audio=bidirectional_audio)\n",
    "\n",
    "    inp_shape_1 = (None,memlen+1,19)\n",
    "\n",
    "    sym_inp = Input(shape = inp_shape_1[1:], batch_size = batch_size)\n",
    "\n",
    "    return train_gen, test_gen, sym_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00bdcb-30da-4753-92c3-ef67c7af59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen, test_gen, sym_inp = get_inputs_and_gens(train_txt_fp,\n",
    "                                                      test_txt_fp, \n",
    "                                                      shuffle, \n",
    "                                                      batch_size=batch_size, \n",
    "                                                      memlen=memlen)\n",
    "\n",
    "sym_proc = layers.LSTM(128, return_sequences = True, dropout = .5)(sym_inp)\n",
    "sym_proc = layers.LSTM(128, return_sequences = False, dropout = .5)(sym_proc)\n",
    "\n",
    "output = layers.Dense(256, activation = 'softmax')(sym_proc)\n",
    "\n",
    "model = Model(sym_inp, output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=1e0),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False),\n",
    "    metrics=[\n",
    "    tf.keras.metrics.AUC(from_logits = False, curve = 'PR', name = 'auc'),\n",
    "    tf.keras.metrics.F1Score(average = 'micro'),\n",
    "    tf.keras.metrics.CategoricalAccuracy()\n",
    "],\n",
    ")\n",
    "print(model.summary())\n",
    "checkpoint_filepath = os.path.join(model_dir, 'sym_ddc_checkpoint.keras')\n",
    "if load_checkpoint:\n",
    "    if os.path.isfile(checkpoint_filepath):\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose = 0,\n",
    "    save_best_only = True,\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max')\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    mode = 'max',\n",
    "    start_from_epoch = 100\n",
    ")\n",
    "\n",
    "model.fit(train_gen, \n",
    "          batch_size = batch_size, \n",
    "          epochs = nepochs, \n",
    "          steps_per_epoch = steps_per_epoch, \n",
    "          validation_steps = 20, \n",
    "          validation_data = test_gen, \n",
    "          callbacks = [model_checkpoint_callback, lr_scheduler, early_stopping])\n",
    "\n",
    "model.save(model_dir + '/sym_ddc_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c046f-89d9-4b7d-b2a6-3b1dadd18be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
