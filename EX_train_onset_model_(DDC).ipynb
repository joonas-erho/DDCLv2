{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f3e7a-752d-4d29-aed9-21bf759b976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model, Input\n",
    "import numpy as np\n",
    "from util import *\n",
    "import os\n",
    "import gc\n",
    "import copy as c\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f440617-15f0-4f48-8656-94ee1792bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "seed = 420\n",
    "batch_size = 256\n",
    "memlen = 100\n",
    "context_radius = 7\n",
    "steps_per_epoch = 200\n",
    "nepochs = 1000\n",
    "nmelbands = 80\n",
    "nchannels = 3\n",
    "npred_steps = 5\n",
    "mem_size = 2500\n",
    "load_checkpoint = False\n",
    "name_from_fp = lambda x: os.path.splitext(os.path.split(x)[1])[0]\n",
    "model_dir = 'old_trained_models'\n",
    "train_txt_fp = 'json/songs/songs_train.txt'\n",
    "test_txt_fp = 'json/songs/songs_test.txt'\n",
    "feats_dir = 'feats/songs'\n",
    "diff_dict = {\n",
    "    'Beginner': 0,\n",
    "    'Easy': 1,\n",
    "    'Medium': 2,\n",
    "    'Hard': 3,\n",
    "    'Challenge': 4,\n",
    "    'Edit': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21ecf10-35e3-4570-9c74-c780b64d8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorify_from_fp_list(dataset_fp, \n",
    "                              memlen = 100, \n",
    "                              batch_size = 50, \n",
    "                              mem_size = 10000, \n",
    "                              shuffle = False):\n",
    "    def _gener():\n",
    "        k = 0\n",
    "        hopper = 0\n",
    "        song = None\n",
    "        song_feats = None\n",
    "        with open(dataset_fp, 'r') as f:\n",
    "            json_fps = f.read().splitlines()\n",
    "            json_fps = list(np.unique(json_fps))\n",
    "        np.random.seed(seed)\n",
    "        json_fps = list(np.random.permutation(json_fps))\n",
    "        while True:\n",
    "            while hopper < mem_size:\n",
    "                song = None\n",
    "                hopper = 0\n",
    "                json_fp = json_fps[k]\n",
    "                k = (k + 1) % (len(json_fps) - 1)\n",
    "                with open(json_fp, 'r') as json_f:\n",
    "                    meta = json.loads(json_f.read())\n",
    "                json_name = name_from_fp(json_fp)\n",
    "                song_feats_fp = os.path.join(feats_dir, '{}.pkl'.format(json_name))\n",
    "                with open(song_feats_fp, 'rb') as f:\n",
    "                    song_feats = pickle.load(f)\n",
    "\n",
    "                newsong = [[],[],[]]\n",
    "\n",
    "                for chart in meta['charts']:\n",
    "                    if not chart['type'] or chart['type'] != 'dance-double':\n",
    "                        placed_notes = []\n",
    "                        for note in chart['notes']:\n",
    "                            if note[3] != '0000':\n",
    "                                placed_notes.append(int(round(note[2]*100)))\n",
    "                        for j in range(0,len(song_feats),npred_steps):\n",
    "                            newsong[0]+= [j]\n",
    "                            newsong[1].append([[diff_dict[chart['difficulty_coarse']]] for _ in range(memlen+npred_steps-1)])\n",
    "                            stick_on = []\n",
    "                            for i in range(j,j+5):\n",
    "                                if i in placed_notes:\n",
    "                                    stick_on.append([1])\n",
    "                                else:\n",
    "                                    stick_on.append([0])\n",
    "                            newsong[2].append(stick_on)\n",
    "\n",
    "                if song is None:\n",
    "                    song = newsong\n",
    "                else:\n",
    "                    for j in range(3):\n",
    "                        song[j] = np.append(song[j], newsong[j], axis = 0)\n",
    "                hopper += len(newsong[0])\n",
    "\n",
    "                if shuffle == True:\n",
    "                    for i in range(3):\n",
    "                        np.random.seed(seed)\n",
    "                        song[i] = np.random.permutation(song[i])\n",
    "                gc.collect()\n",
    "            gc.collect()\n",
    "                \n",
    "            assert len(song[0])>5*batch_size\n",
    "\n",
    "            ac = []\n",
    "            sd = []\n",
    "            lb = []\n",
    "            for i in range(0,npred_steps*batch_size,npred_steps):\n",
    "                ac.append(make_onset_feature_context(song_feats, song[0][i], radius = 3+npred_steps, left_radius = memlen+3))\n",
    "                sd.append(song[1][i])\n",
    "                lb.append(song[2][i])\n",
    "                \n",
    "            ac, sd, lb = np.array(ac), np.array(sd), np.squeeze(np.array(lb))\n",
    "            \n",
    "            for j in range(3):\n",
    "                song[j] = song[j][int(npred_steps*batch_size):]\n",
    "            assert(len(song[0])==len(song[1]) and len(song[1])==len(song[2]))\n",
    "            hopper -= npred_steps*batch_size\n",
    "            gc.collect()\n",
    "            yield (ac,sd), lb\n",
    "    return _gener()\n",
    "\n",
    "def get_inputs_and_gens(trn_fp, tst_fp, shuffle = False, batch_size = 1000, memlen = 8, mem_size = 2500):\n",
    "\n",
    "    inp_shape_0 = (None,memlen + 7 + npred_steps ,nmelbands,nchannels)\n",
    "    inp_shape_1 = (None,memlen+npred_steps-1,1)\n",
    "\n",
    "    train_gen = generatorify_from_fp_list(trn_fp, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle = shuffle, \n",
    "                                          mem_size=mem_size,\n",
    "                                          memlen = memlen)\n",
    "    test_gen = generatorify_from_fp_list(tst_fp, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle = shuffle, \n",
    "                                         mem_size=mem_size,\n",
    "                                         memlen = memlen)\n",
    "\n",
    "    audio_ctx_inp = Input(shape = inp_shape_0[1:], batch_size = batch_size)\n",
    "    stream_inp = Input(shape = inp_shape_1[1:], batch_size = batch_size)\n",
    "\n",
    "    return train_gen, test_gen, audio_ctx_inp, stream_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c02b06-9725-425b-bec7-5efbac86221b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "train_gen, test_gen, audio_ctx_inp, stream_inp= get_inputs_and_gens(train_txt_fp, \n",
    "                                                                  test_txt_fp,\n",
    "                                                                  shuffle, \n",
    "                                                                  batch_size=batch_size, \n",
    "                                                                  memlen = memlen,\n",
    "                                                                   mem_size = mem_size)\n",
    "\n",
    "audio_proc = layers.BatchNormalization()(audio_ctx_inp)\n",
    "audio_proc = layers.Conv2D(10, (7,3))(audio_proc)\n",
    "audio_proc = layers.MaxPooling2D((1,3), strides = (1,3))(audio_proc)\n",
    "audio_proc = layers.Conv2D(20, (3,3))(audio_proc)\n",
    "audio_proc = layers.MaxPooling2D((1,3), strides = (1,3))(audio_proc)\n",
    "\n",
    "audio_out = layers.Reshape((memlen+npred_steps-1,-1))(audio_proc)\n",
    "\n",
    "stream_merge = layers.Concatenate(axis = -1)([audio_out, stream_inp])\n",
    "\n",
    "note_comp = layers.LSTM(200, return_sequences = True, dropout = .5)(stream_merge)\n",
    "note_comp = layers.LSTM(200, dropout = .5)(note_comp)\n",
    "\n",
    "note_comp = layers.Dense(256, activation = 'relu')(note_comp)\n",
    "note_comp = layers.Dense(128, activation = 'relu')(note_comp)\n",
    "\n",
    "output = layers.Dense(npred_steps, activation = 'sigmoid')(note_comp)\n",
    "\n",
    "model = Model([audio_ctx_inp, stream_inp], output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    #optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-2, clipvalue = 5),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "    metrics=[\n",
    "    tf.keras.metrics.AUC(from_logits = False, curve = 'PR', name = 'auc'),\n",
    "    tf.keras.metrics.F1Score(average = 'micro', threshold = .5, name = 'f1'),\n",
    "    tf.keras.metrics.BinaryAccuracy(name = 'acc'),\n",
    "],\n",
    ")\n",
    "print(model.summary())\n",
    "checkpoint_filepath = os.path.join(model_dir, 'onset_ddc_checkpoint.keras')\n",
    "if load_checkpoint:\n",
    "    if os.path.isfile(checkpoint_filepath):\n",
    "        print(True)\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose = 0,\n",
    "    save_best_only = True,\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max')\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    mode = 'max',\n",
    "    start_from_epoch = 100\n",
    ")\n",
    "\n",
    "model.fit(train_gen, \n",
    "          batch_size = batch_size, \n",
    "          epochs = nepochs, \n",
    "          steps_per_epoch = steps_per_epoch, \n",
    "          validation_steps = 20, \n",
    "          validation_data = test_gen, \n",
    "          callbacks = [model_checkpoint_callback, lr_scheduler, early_stopping])\n",
    "\n",
    "model.save(model_dir + '/onset_ddc_model.keras')\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec57f55-abe0-42e0-87d6-e6a4d50e38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
